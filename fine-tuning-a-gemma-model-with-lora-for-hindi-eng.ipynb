{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10022647,"sourceType":"datasetVersion","datasetId":6171858},{"sourceId":85986,"sourceType":"modelInstanceVersion","modelInstanceId":72246,"modelId":78150}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/swapanroy/fine-tuning-a-gemma-model-with-lora-for-hindi-eng?scriptVersionId=220799501\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Fine-Tuning the Gemma2_instruct_2b_en Model with LoRA for Hindi-English Translation\n\n(Low-Rank Adaptation)","metadata":{}},{"cell_type":"markdown","source":"## Install dependent libraries ","metadata":{}},{"cell_type":"code","source":"!pip install -q -U keras-nlp datasets\n!pip install -q -U keras\nprint(\"Installation Complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T13:58:54.203477Z","iopub.execute_input":"2025-02-04T13:58:54.203883Z","iopub.status.idle":"2025-02-04T13:59:13.392029Z","shell.execute_reply.started":"2025-02-04T13:58:54.203844Z","shell.execute_reply":"2025-02-04T13:59:13.390892Z"}},"outputs":[{"name":"stdout","text":"Installation Complete\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import keras\nimport keras_nlp\nimport os\n\n# Set the backend to JAX for efficient tensor computations\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n# Allocate 90% of available memory to JAX to avoid fragmentation\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.9\"\n\n#KerasNLP is a natural language processing library that works natively with JAX\n\nimport keras_nlp\nimport keras\n\n# Configure Keras to use half-precision floating-point numbers (bfloat16)\n# for faster training and reduced memory usage\n# keras.config.set_floatx(\"bfloat16\")\n\n# Training hyperparameters\ntoken_limit = 254  # Maximum sequence length\nnum_data_limit = 100  # Number of training examples\nlora_name = \"Roy\"  # Name of the LoRA model\nlora_rank = 4  # Rank of the LoRA model\nlr_value = 1e-4  # Learning rate\ntrain_epoch = 20  # Number of training epochs\n#model_id = \"gemma2_instruct_2b_en\"  # Base model ID; ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T14:56:08.369288Z","iopub.execute_input":"2025-02-04T14:56:08.369711Z","iopub.status.idle":"2025-02-04T14:56:08.376307Z","shell.execute_reply.started":"2025-02-04T14:56:08.369638Z","shell.execute_reply":"2025-02-04T14:56:08.375423Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### LOAD Model","metadata":{}},{"cell_type":"code","source":"\nimport time\n\n# Import the GemmaCausalLM model from KerasNLP\n#gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset('gemma2_instruct_2b_en')\n\n# Print a summary of the model's architecture\ngemma_lm.summary()\n\n# Initialize a variable to track the start time\ntick_start = 0\n\n# Function to start, stop and print the timer\ndef tick():\n    global tick_start\n    tick_start = time.time()\n\ndef tock():\n    print(f\"TOTAL TIME ELAPSED: {time.time() - tick_start:.2f}s\")\n\ndef text_gen(prompt):\n    tick()\n    input = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n    output = gemma_lm.generate(input, max_length=token_limit)\n    print(\"\\nGemma output:\")\n    print(output)\n    tock()\n\n# inference before fine-tuning\n\n# Generate text for the first prompt\ntext_gen(\"Do you know Hindi language?\")\n\n# Generate text for the second prompt (in Hindi)\ntext_gen(\"क्या आप हिंदी भाषा जानते हैं?\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T15:03:01.594813Z","iopub.execute_input":"2025-02-04T15:03:01.595635Z","iopub.status.idle":"2025-02-04T15:04:28.009865Z","shell.execute_reply.started":"2025-02-04T15:03:01.595604Z","shell.execute_reply":"2025-02-04T15:04:28.00897Z"}},"outputs":[{"name":"stderr","text":"normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,614,341,888\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1738681448.697148      30 service.cc:145] XLA service 0x582fcaff0ad0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1738681448.697205      30 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1738681461.759151      30 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\nGemma output:\n<start_of_turn>user\nDo you know Hindi language?<end_of_turn>\n<start_of_turn>model\nAs an AI, I don't \"know\" languages in the way a human does. I can't speak or understand Hindi in the same way you do. \n\nHowever, I have been trained on a massive dataset of text and code, which includes a significant amount of Hindi. This means I can:\n\n* **Translate** Hindi to English and vice versa.\n* **Generate** text in Hindi, like writing simple sentences or stories.\n* **Identify** Hindi words and phrases.\n* **Answer** questions about Hindi grammar and vocabulary.\n\nSo, while I don't \"know\" Hindi in a human sense, I can still be helpful with it! \n\nHow can I help you with Hindi today? 😊 \n<end_of_turn>\nTOTAL TIME ELAPSED: 27.44s\n\nGemma output:\n<start_of_turn>user\nक्या आप हिंदी भाषा जानते हैं?<end_of_turn>\n<start_of_turn>model\nहाँ, मैं हिंदी भाषा जानता हूँ! 😊  \n\nआप मुझसे हिंदी में बात कर सकते हैं। आप मुझे क्या पूछना चाहते हैं? \n<end_of_turn>\nTOTAL TIME ELAPSED: 1.36s\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import keras\nimport keras_nlp\nimport datasets\n\n# Load a pre-trained Gemma tokenizer\n\ntokenizer = keras_nlp.models.GemmaTokenizer.from_preset(\"gemma2_instruct_2b_en\")\n\n## Load the Hindi-English translation dataset, this is they key to how good our model will work\nfrom datasets import load_dataset\nds = load_dataset(\n    \"/kaggle/input/iit-b-hindi/\",\n    split=\"train\",\n)\nprint(ds)\ndata = ds.with_format(\"np\", columns=[\"hindi\", \"english\"], output_all_columns=False)\ntrain = []\nfor x in data:\n  item = f\"<start_of_turn>user\\nनिचले पटल के लिए डिफोल्ट प्लग-इन खाका.\\n\\\"{x['hindi']}\\\"<end_of_turn>\\n<start_of_turn>model\\n{x['english']}<end_of_turn>\"\n  length = len(tokenizer(item))\n  # skip data if the token length is longer than our limit\n  if length < token_limit:\n    train.append(item)\n    if(len(train)>=num_data_limit):\n      break\n\n# Print the number of training examples and the first few examples\nprint(len(train))\nprint(train[0])\nprint(train[1])\nprint(train[2])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T15:10:04.519032Z","iopub.execute_input":"2025-02-04T15:10:04.519387Z","iopub.status.idle":"2025-02-04T15:10:20.088337Z","shell.execute_reply.started":"2025-02-04T15:10:04.519356Z","shell.execute_reply":"2025-02-04T15:10:20.087424Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6abebcc89784b87a5073c718c4370a0"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['hindi', 'english'],\n    num_rows: 1561841\n})\n100\n<start_of_turn>user\nनिचले पटल के लिए डिफोल्ट प्लग-इन खाका.\n\"अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें\"<end_of_turn>\n<start_of_turn>model\nGive your application an accessibility workout<end_of_turn>\n<start_of_turn>user\nनिचले पटल के लिए डिफोल्ट प्लग-इन खाका.\n\"एक्सेर्साइसर पहुंचनीयता अन्वेषक\"<end_of_turn>\n<start_of_turn>model\nAccerciser Accessibility Explorer<end_of_turn>\n<start_of_turn>user\nनिचले पटल के लिए डिफोल्ट प्लग-इन खाका.\n\"निचले पटल के लिए डिफोल्ट प्लग-इन खाका\"<end_of_turn>\n<start_of_turn>model\nThe default plugin layout for the bottom panel<end_of_turn>\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Enable LoRA for the model and set the LoRA rank to 4.\ngemma_lm.backbone.enable_lora(rank=lora_rank)\n\n# Print a summary of the model, including LoRA parameters\ngemma_lm.summary()\n\n# Limit the input sequence length (to control memory usage).\ngemma_lm.preprocessor.sequence_length = token_limit\n\n# Configure the AdamW optimizer with weight decay\noptimizer = keras.optimizers.AdamW(\n    learning_rate=lr_value,\n    weight_decay=0.01,\n)\n# Exclude bias and scale parameters from weight decay\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\n# Compile the model with sparse categorical crossentropy loss and AdamW optimizer\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T15:10:57.206268Z","iopub.execute_input":"2025-02-04T15:10:57.207246Z","iopub.status.idle":"2025-02-04T15:10:57.407593Z","shell.execute_reply.started":"2025-02-04T15:10:57.207212Z","shell.execute_reply":"2025-02-04T15:10:57.406786Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,617,270,528\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,617,270,528\u001b[0m (9.75 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> (9.75 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,928,640\u001b[0m (11.17 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,928,640</span> (11.17 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n</pre>\n"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"### Custom callback for Keras that saves LoRA weights and performs basic evaluation after each training epoch.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nclass CustomCallback(keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs=None):\n    model_name = f\"/kaggle/working/{lora_name}_{lora_rank}_epoch{epoch+1}.lora.h5\"\n    gemma_lm.backbone.save_lora_weights(model_name)\n\n    # Evaluate\n    text_gen(\"निचले पटल के लिए डिफोल्ट प्लग-इन खाका\")\n\n# Train the model with the custom callback\nhistory = gemma_lm.fit(train, epochs=train_epoch, batch_size=1, callbacks=[CustomCallback()])\n\n# Plot training history\n\nplt.plot(history.history['loss'])\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T15:11:14.163525Z","iopub.execute_input":"2025-02-04T15:11:14.164214Z","iopub.status.idle":"2025-02-04T15:27:02.500252Z","shell.execute_reply.started":"2025-02-04T15:11:14.164183Z","shell.execute_reply":"2025-02-04T15:27:02.499431Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1738681923.265042      97 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1738681942.015579      97 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_9', 436 bytes spill stores, 508 bytes spill loads\nptxas warning : Registers are spilled to local memory in function '__cuda_sm3x_div_rn_noftz_f32_slowpath', 24 bytes spill stores, 24 bytes spill loads\nptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_7', 68 bytes spill stores, 116 bytes spill loads\nptxas warning : Registers are spilled to local memory in function '__cuda_sm3x_div_rn_noftz_f32_slowpath', 12 bytes spill stores, 12 bytes spill loads\nptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_5', 436 bytes spill stores, 508 bytes spill loads\nptxas warning : Registers are spilled to local memory in function '__cuda_sm3x_div_rn_noftz_f32_slowpath', 24 bytes spill stores, 24 bytes spill loads\n\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424ms/step - loss: 0.9832 - sparse_categorical_accuracy: 0.2849\nGemma output:\n<start_of_turn>user\nनिचले पटल के लिए डिफोल्ट प्लग-इन खाका<end_of_turn>\n<start_of_turn>model\nनिचले पटल<end_of_turn>\nTOTAL TIME ELAPSED: 22.84s\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 656ms/step - loss: 0.9808 - sparse_categorical_accuracy: 0.2860\nEpoch 2/20\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - loss: 0.3454 - sparse_categorical_accuracy: 0.7449\nGemma output:\n<start_of_turn>user\nनिचले पटल के लिए डिफोल्ट प्लग-इन खाका<end_of_turn>\n<start_of_turn>model\n\"Default\"<end_of_turn>\nTOTAL TIME ELAPSED: 0.35s\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 428ms/step - loss: 0.3447 - sparse_categorical_accuracy: 0.7453\nEpoch 3/20\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - loss: 0.2526 - sparse_categorical_accuracy: 0.8015\nGemma output:\n<start_of_turn>user\nनिचले पटल के लिए डिफोल्ट प्लग-इन खाका<end_of_turn>\n<start_of_turn>model\n\"Standard\"<end_of_turn>\nTOTAL TIME ELAPSED: 0.35s\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 427ms/step - loss: 0.2522 - sparse_categorical_accuracy: 0.8017\nEpoch 4/20\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - loss: 0.2164 - sparse_categorical_accuracy: 0.8222\nGemma output:\n<start_of_turn>user\nनिचले पटल के लिए डिफोल्ट प्लग-इन खाका<end_of_turn>\n<start_of_turn>model\n\"Selection\"<end_of_turn>\nTOTAL TIME ELAPSED: 0.35s\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 428ms/step - loss: 0.2160 - sparse_categorical_accuracy: 0.8224\nEpoch 5/20\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - loss: 0.1894 - sparse_categorical_accuracy: 0.8337\nGemma output:\n<start_of_turn>user\nनिचले पटल के लिए डिफोल्ट प्लग-इन खाका<end_of_turn>\n<start_of_turn>model\n\"Default plugin layout\"<end_of_turn>\nTOTAL TIME ELAPSED: 0.42s\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 428ms/step - loss: 0.1891 - sparse_categorical_accuracy: 0.8339\nEpoch 6/20\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - loss: 0.1659 - sparse_categorical_accuracy: 0.8516\nGemma output:\n<start_of_turn>user\nनिचले पटल के लिए डिफोल्ट प्लग-इन खाका<end_of_turn>\n<start_of_turn>model\n\"Default plugin layout\"<end_of_turn>\nTOTAL TIME ELAPSED: 0.42s\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 428ms/step - loss: 0.1656 - sparse_categorical_accuracy: 0.8517\nEpoch 7/20\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - loss: 0.1427 - sparse_categorical_accuracy: 0.8689\nGemma output:\n<start_of_turn>user\nनिचले पटल के लिए डिफोल्ट प्लग-इन खाका<end_of_turn>\n<start_of_turn>model\nThe default plugin layout<end_of_turn>\nTOTAL TIME ELAPSED: 0.39s\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 428ms/step - loss: 0.1425 - sparse_categorical_accuracy: 0.8690\nEpoch 8/20\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - loss: 0.1221 - sparse_categorical_accuracy: 0.8879\nGemma output:\n<start_of_turn>user\nनिचले पटल के लिए डिफोल्ट प्लग-इन खाका<end_of_turn>\n<start_of_turn>model\nChoi _ ni niuchi mune<end_of_turn>\nTOTAL TIME ELAPSED: 0.49s\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 429ms/step - loss: 0.1219 - sparse_categorical_accuracy: 0.8879\nEpoch 9/20\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - loss: 0.1054 - sparse_categorical_accuracy: 0.9038\nGemma output:\n<start_of_turn>user\nनिचले पटल के लिए डिफोल्ट प्लग-इन खाका<end_of_turn>\n<start_of_turn>model\nThe default plugin layout for the current panel<end_of_turn>\nTOTAL TIME ELAPSED: 0.53s\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 429ms/step - loss: 0.1053 - sparse_categorical_accuracy: 0.9038\nEpoch 10/20\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - loss: 0.0908 - sparse_categorical_accuracy: 0.9143\nGemma output:\n<start_of_turn>user\nनिचले पटल के लिए डिफोल्ट प्लग-इन खाका<end_of_turn>\n<start_of_turn>model\nThe default plugin for the current PyQt session<end_of_turn>\nTOTAL TIME ELAPSED: 0.53s\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 429ms/step - loss: 0.0907 - sparse_categorical_accuracy: 0.9143\nEpoch 11/20\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - loss: 0.0786 - sparse_categorical_accuracy: 0.9280\nGemma output:\n<start_of_turn>user\nनिचले पटल के लिए डिफोल्ट प्लग-इन खाका<end_of_turn>\n<start_of_turn>model\nThe default plugin layout for the bottom panel<end_of_turn>\nTOTAL TIME ELAPSED: 0.53s\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 429ms/step - loss: 0.0785 - sparse_categorical_accuracy: 0.9280\nEpoch 12/20\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - loss: 0.0704 - sparse_categorical_accuracy: 0.9353\nGemma output:\n<start_of_turn>user\nनिचले पटल के लिए डिफोल्ट प्लग-इन खाका<end_of_turn>\n<start_of_turn>model\nThe default plugin layout<end_of_turn>\nTOTAL TIME ELAPSED: 0.39s\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 428ms/step - loss: 0.0703 - sparse_categorical_accuracy: 0.9353\nEpoch 13/20\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - loss: 0.0630 - sparse_categorical_accuracy: 0.9394\nGemma output:\n<start_of_turn>user\nनिचले पटल के लिए डिफोल्ट प्लग-इन खाका<end_of_turn>\n<start_of_turn>model\n{}<end_of_turn>\nTOTAL TIME ELAPSED: 0.28s\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 427ms/step - loss: 0.0630 - sparse_categorical_accuracy: 0.9394\nEpoch 14/20\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - loss: 0.0582 - sparse_categorical_accuracy: 0.9411\nGemma output:\n<start_of_turn>user\nनिचले पटल के लिए डिफोल्ट प्लग-इन खाका<end_of_turn>\n<start_of_turn>model\n{}<end_of_turn>\nTOTAL TIME ELAPSED: 0.28s\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 427ms/step - loss: 0.0581 - sparse_categorical_accuracy: 0.9411\nEpoch 15/20\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - loss: 0.0562 - sparse_categorical_accuracy: 0.9418\nGemma output:\n<start_of_turn>user\nनिचले पटल के लिए डिफोल्ट प्लग-इन खाका<end_of_turn>\n<start_of_turn>model\nCustom plugin<end_of_turn>\nTOTAL TIME ELAPSED: 0.31s\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 427ms/step - loss: 0.0562 - sparse_categorical_accuracy: 0.9417\nEpoch 16/20\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - loss: 0.0554 - sparse_categorical_accuracy: 0.9400\nGemma output:\n<start_of_turn>user\nनिचले पटल के लिए डिफोल्ट प्लग-इन खाका<end_of_turn>\n<start_of_turn>model\nWidely available<end_of_turn>\nTOTAL TIME ELAPSED: 0.35s\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 427ms/step - loss: 0.0554 - sparse_categorical_accuracy: 0.9400\nEpoch 17/20\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - loss: 0.0518 - sparse_categorical_accuracy: 0.9411\nGemma output:\n<start_of_turn>user\nनिचले पटल के लिए डिफोल्ट प्लग-इन खाका<end_of_turn>\n<start_of_turn>model\nWidely available<end_of_turn>\nTOTAL TIME ELAPSED: 0.35s\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 427ms/step - loss: 0.0518 - sparse_categorical_accuracy: 0.9411\nEpoch 18/20\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - loss: 0.0485 - sparse_categorical_accuracy: 0.9418\nGemma output:\n<start_of_turn>user\nनिचले पटल के लिए डिफोल्ट प्लग-इन खाका<end_of_turn>\n<start_of_turn>model\nWidely available<end_of_turn>\nTOTAL TIME ELAPSED: 0.35s\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 427ms/step - loss: 0.0485 - sparse_categorical_accuracy: 0.9418\nEpoch 19/20\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - loss: 0.0467 - sparse_categorical_accuracy: 0.9421\nGemma output:\n<start_of_turn>user\nनिचले पटल के लिए डिफोल्ट प्लग-इन खाका<end_of_turn>\n<start_of_turn>model\nWidely available<end_of_turn>\nTOTAL TIME ELAPSED: 0.35s\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 427ms/step - loss: 0.0467 - sparse_categorical_accuracy: 0.9420\nEpoch 20/20\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - loss: 0.0443 - sparse_categorical_accuracy: 0.9423\nGemma output:\n<start_of_turn>user\nनिचले पटल के लिए डिफोल्ट प्लग-इन खाका<end_of_turn>\n<start_of_turn>model\nWidely available<end_of_turn>\nTOTAL TIME ELAPSED: 0.35s\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 427ms/step - loss: 0.0442 - sparse_categorical_accuracy: 0.9422\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Example Code for Load LoRA\n\nimport os\nimport keras\nimport keras_nlp\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset('gemma2_instruct_2b_en')\n\n# Use the same LoRA rank that you trained, \n# https://keras.io/keras_hub/api/models/gemma/gemma_backbone/#gemmabackbone-class\ngemma_lm.backbone.enable_lora(rank=4)\n\n# Load pre-trained LoRA weights\ngemma_lm.backbone.load_lora_weights(f\"/kaggle/working/Roy_4_epoch19.lora.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T15:32:39.57184Z","iopub.execute_input":"2025-02-04T15:32:39.572236Z","iopub.status.idle":"2025-02-04T15:32:52.195055Z","shell.execute_reply.started":"2025-02-04T15:32:39.572207Z","shell.execute_reply":"2025-02-04T15:32:52.193525Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m gemma_lm \u001b[38;5;241m=\u001b[39m \u001b[43mkeras_nlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGemmaCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_preset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgemma2_instruct_2b_en\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Use the same LoRA rank that you trained, \u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# https://keras.io/keras_hub/api/models/gemma/gemma_backbone/#gemmabackbone-class\u001b[39;00m\n\u001b[1;32m     10\u001b[0m gemma_lm\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39menable_lora(rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/models/task.py:202\u001b[0m, in \u001b[0;36mTask.from_preset\u001b[0;34m(cls, preset, load_weights, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Specifically for classifiers, we never load task weights if\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# num_classes is supplied. We handle this in the task base class because\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# it is the same logic for classifiers regardless of modality (text,\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# images, audio).\u001b[39;00m\n\u001b[1;32m    201\u001b[0m load_task_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_task\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_task_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/utils/preset_utils.py:753\u001b[0m, in \u001b[0;36mKerasPresetLoader.load_task\u001b[0;34m(self, cls, load_weights, load_task_weights, **kwargs)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_task\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mcls\u001b[39m, load_weights, load_task_weights, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;66;03m# If there is no `task.json` or it's for the wrong class delegate to the\u001b[39;00m\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;66;03m# super class loader.\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_file_exists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreset, TASK_CONFIG_FILE):\n\u001b[0;32m--> 753\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_task_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    756\u001b[0m     task_config \u001b[38;5;241m=\u001b[39m load_json(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreset, TASK_CONFIG_FILE)\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(check_config_class(task_config), \u001b[38;5;28mcls\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/utils/preset_utils.py:695\u001b[0m, in \u001b[0;36mPresetLoader.load_task\u001b[0;34m(self, cls, load_weights, load_task_weights, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;66;03m# Forward dtype to backbone.\u001b[39;00m\n\u001b[1;32m    694\u001b[0m     backbone_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)}\n\u001b[0;32m--> 695\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_backbone\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackbone_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbackbone_kwargs\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    699\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_preprocessor(\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor_cls,\n\u001b[1;32m    701\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/utils/preset_utils.py:729\u001b[0m, in \u001b[0;36mKerasPresetLoader.load_backbone\u001b[0;34m(self, cls, load_weights, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_backbone\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mcls\u001b[39m, load_weights, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 729\u001b[0m     backbone \u001b[38;5;241m=\u001b[39m \u001b[43mload_serialized_object\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m load_weights:\n\u001b[1;32m    731\u001b[0m         jax_memory_cleanup(backbone)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/utils/preset_utils.py:580\u001b[0m, in \u001b[0;36mload_serialized_object\u001b[0;34m(config, **kwargs)\u001b[0m\n\u001b[1;32m    577\u001b[0m config \u001b[38;5;241m=\u001b[39m set_dtype_in_config(config, dtype)\n\u001b[1;32m    579\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaving\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:718\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m custom_obj_scope, safe_mode_scope:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    721\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m could not be deserialized properly. Please\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    722\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ensure that components that are Python object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    727\u001b[0m         )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/models/backbone.py:139\u001b[0m, in \u001b[0;36mBackbone.from_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_config\u001b[39m(\u001b[38;5;28mcls\u001b[39m, config):\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# The default `from_config()` for functional models will return a\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# vanilla `keras.Model`. We override it to get a subclass instance back.\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/models/gemma/gemma_backbone.py:171\u001b[0m, in \u001b[0;36mGemmaBackbone.__init__\u001b[0;34m(self, vocabulary_size, num_layers, num_query_heads, num_key_value_heads, hidden_dim, intermediate_dim, head_dim, query_head_dim_normalize, use_post_ffw_norm, use_post_attention_norm, attention_logit_soft_cap, final_logit_soft_cap, use_sliding_window_attention, sliding_window_size, layer_norm_epsilon, dropout, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m token_id_input \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mInput(\n\u001b[1;32m    166\u001b[0m     shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m )\n\u001b[1;32m    168\u001b[0m padding_mask_input \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mInput(\n\u001b[1;32m    169\u001b[0m     shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpadding_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m )\n\u001b[0;32m--> 171\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_id_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m ops\u001b[38;5;241m.\u001b[39mcast(ops\u001b[38;5;241m.\u001b[39msqrt(hidden_dim), x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transformer_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_layers:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/layers/modeling/reversible_embedding.py:117\u001b[0m, in \u001b[0;36mReversibleEmbedding.build\u001b[0;34m(self, inputs_shape)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtie_weights\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m     ):\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreverse_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_weight(\n\u001b[1;32m    123\u001b[0m             name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreverse_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    124\u001b[0m             shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim),\n\u001b[1;32m    125\u001b[0m             initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_initializer,\n\u001b[1;32m    126\u001b[0m             dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    127\u001b[0m         )\n","\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:AddV2] name: "],"ename":"ResourceExhaustedError","evalue":"{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:AddV2] name: ","output_type":"error"}],"execution_count":12},{"cell_type":"markdown","source":"### Compile the model with a top-k and top-p sampling strategy and Generate text using the compiled model\n\n#### Influence of Parameter *k* on OutputThe parameter *k* directly controls the diversity of the generated text:\n\n\n* **Low *k*** (e.g., *k* = 1): With *k* = 1, top-k sampling behaves like greedy decoding, as only the most probable token is selected deterministically, resulting in a predictable and repetitive output.\n* **Moderate *k*** (e.g., *k* = 5–10): Small values of *k* introduce moderate randomness by restricting selection to the most probable tokens while maintaining context fidelity. This setting is useful when balanced diversity is desired, as it reduces the likelihood of incoherent or tangential token choices.\n* **High *k*** (e.g., *k* > 20): Larger values of *k* allow more diversity, as the model can choose from a broader set of tokens. However, this can also increase the risk of generating less coherent text, as the inclusion of lower-probability tokens may lead to irrelevant or unexpected output.The optimal value of *k* often depends on the specific task and the level of creativity or strictness required in the generated text.\n\n\nhttps://dataforest.ai/glossary/top-k-sampling#:~:text=However%2C%20this%20can%20also%20increase,lead%20to%20irrelevant%20or%20unexpected","metadata":{}},{"cell_type":"code","source":"gemma_lm.compile(sampler=\"top_k\") #default = 50\n\ntext_gen(\"कृपया एक ईमेल उत्तर लिखें:\\n\\\"हैलो, मैं हमारी शादी की सालगिरह के लिए एक केक नंबर 3 ऑर्डर करना चाहूंगा?\")\ntext_gen(\"कृपया निम्नलिखित के लिए एक ईमेल उत्तर लिखें।\\n\\\"हैलो, मैं हमारी शादी की सालगिरह के लिए एक केक नंबर 3 ऑर्डर करना चाहूंगा। क्या यह संभव है?\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T15:52:39.918886Z","iopub.execute_input":"2025-02-04T15:52:39.919246Z","iopub.status.idle":"2025-02-04T15:53:09.329594Z","shell.execute_reply.started":"2025-02-04T15:52:39.919217Z","shell.execute_reply":"2025-02-04T15:53:09.328722Z"}},"outputs":[{"name":"stdout","text":"\nGemma output:\n<start_of_turn>user\nकृपया एक ईमेल उत्तर लिखें:\n\"हैलो, मैं हमारी शादी की सालगिरह के लिए एक केक नंबर 3 ऑर्डर करना चाहूंगा?<end_of_turn>\n<start_of_turn>model\nHello! We'd be delighted to order a cake for your anniversary. How many people would be in the party? What flavors do you prefer?<end_of_turn>\nTOTAL TIME ELAPSED: 28.21s\n\nGemma output:\n<start_of_turn>user\nकृपया निम्नलिखित के लिए एक ईमेल उत्तर लिखें।\n\"हैलो, मैं हमारी शादी की सालगिरह के लिए एक केक नंबर 3 ऑर्डर करना चाहूंगा। क्या यह संभव है?<end_of_turn>\n<start_of_turn>model\nनमस्ते,\n\nआफ्टर पेज में 3 कोड नंबर ऑर्डर करना संभव है. <end_of_turn>\nTOTAL TIME ELAPSED: 1.18s\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"gemma_lm.compile(sampler=\"top_k\") #default = 50\n\ntext_gen(\"कृपया एक ईमेल उत्तर लिखें:\\n\\\"हैलो, मैं हमारी शादी की सालगिरह के लिए एक केक नंबर 3 ऑर्डर करना चाहूंगा?\")\ntext_gen(\"कृपया एक ईमेल उत्तर सूची का अनुसरण करें। और 10 लोगों के लिए लागत अनुमान पूछें\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T16:07:35.698479Z","iopub.execute_input":"2025-02-04T16:07:35.699203Z","iopub.status.idle":"2025-02-04T16:08:03.725602Z","shell.execute_reply.started":"2025-02-04T16:07:35.699166Z","shell.execute_reply":"2025-02-04T16:08:03.724718Z"}},"outputs":[{"name":"stdout","text":"\nGemma output:\n<start_of_turn>user\nकृपया एक ईमेल उत्तर लिखें:\n\"हैलो, मैं हमारी शादी की सालगिरह के लिए एक केक नंबर 3 ऑर्डर करना चाहूंगा?<end_of_turn>\n<start_of_turn>model\nHello, I would like to place an order for a cake numbe 3 for the anniversary of your wedding? \n<end_of_turn>\nTOTAL TIME ELAPSED: 26.03s\n\nGemma output:\n<start_of_turn>user\nकृपया एक ईमेल उत्तर सूची का अनुसरण करें। और 10 लोगों के लिए लागत अनुमान पूछें<end_of_turn>\n<start_of_turn>model\nविन्यासकर्ता, कृपया अपने विन्यास के लिए अनुप्रयोग का पालन करें। 10 लोगों के लिए लागत अनुमान के लिए डिफोल्ट प्लग-इन का उपयोग करें.<end_of_turn>\nTOTAL TIME ELAPSED: 1.98s\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"\n\n\nhttps://docs.ai-solutions.ext.hpe.com/products/gen-ai/latest/get-started/glossary/model-params-top-p/#:~:text=Set%20a%20lower%20Top%2DP,to%20more%20likely%2C%20predictable%20words.","metadata":{}},{"cell_type":"code","source":"gemma_lm.compile(sampler=\"top_p\") #default=1\n\ntext_gen(\"कृपया बेकर को ईमेल करें और सुनिश्चित करें कि वह ब्लैक फॉरेस्ट केक क्या बना सकता है और हमारी शादी की सालगिरह के लिए 4 मोमबत्तियां दे सकता है और लागत\")\ntext_gen(\"कृपया केक के लिए एक ईमेल उत्तर सूची का अनुसरण करें और 10 लोगों के लिए लागत अनुमान पूछें\")\ntext_gen(\"कृपया एक ईमेल उत्तर लिखें:\\n\\\"हैलो, मैं हमारी शादी की सालगिरह के लिए एक केक नंबर 3 ऑर्डर करना चाहूंगा?\")\ntext_gen(\"कृपया केक के लिए एक ईमेल उत्तर सूची का अनुसरण करें और 10 लोगों के लिए लागत अनुमान के बारे में पूछें, कृपया बांग्ला में प्रतिक्रिया के लिए पूछें।\")\ntext_gen(\"कृपया केक के लिए एक ईमेल उत्तर सूची का अनुसरण करें और 10 लोगों के लिए लागत अनुमान पूछें, कृपया अंग्रेजी में प्रतिक्रिया मांगें\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T16:21:01.935792Z","iopub.execute_input":"2025-02-04T16:21:01.936262Z","iopub.status.idle":"2025-02-04T16:22:04.296455Z","shell.execute_reply.started":"2025-02-04T16:21:01.936219Z","shell.execute_reply":"2025-02-04T16:22:04.294894Z"}},"outputs":[{"name":"stdout","text":"\nGemma output:\n<start_of_turn>user\nकृपया बेकर को ईमेल करें और सुनिश्चित करें कि वह ब्लैक फॉरेस्ट केक क्या बना सकता है और हमारी शादी की सालगिरह के लिए 4 मोमबत्तियां दे सकता है और लागत<end_of_turn>\n<start_of_turn>model\nविषय: शादी सालगिरह के लिए डिफोल्ट प्लग-इन खाका\n\nप्रिय बेकर,\n\nआपकी शादी सालगिरह के लिए डिफोल्ट खाका प्रदान करने के लिए हमने विनंय की. 4 मोमबत्तियां और लागत के साथ डिफोल्ट खाका.\n<end_of_turn>\nTOTAL TIME ELAPSED: 41.19s\n\nGemma output:\n<start_of_turn>user\nकृपया केक के लिए एक ईमेल उत्तर सूची का अनुसरण करें और 10 लोगों के लिए लागत अनुमान पूछें<end_of_turn>\n<start_of_turn>model\nविभाग के लिए प्रारूप का पालन करें और 10 उपयोगकर्ताओं के लिए लागत अनुमान पूछें<end_of_turn>\nTOTAL TIME ELAPSED: 5.96s\n\nGemma output:\n<start_of_turn>user\nकृपया एक ईमेल उत्तर लिखें:\n\"हैलो, मैं हमारी शादी की सालगिरह के लिए एक केक नंबर 3 ऑर्डर करना चाहूंगा?<end_of_turn>\n<start_of_turn>model\nHello, I would like to order a cake number for the anniversary of your wedding.<end_of_turn>\nTOTAL TIME ELAPSED: 3.84s\n\nGemma output:\n<start_of_turn>user\nकृपया केक के लिए एक ईमेल उत्तर सूची का अनुसरण करें और 10 लोगों के लिए लागत अनुमान के बारे में पूछें, कृपया बांग्ला में प्रतिक्रिया के लिए पूछें।<end_of_turn>\n<start_of_turn>model\nপরিचयের 10 ব্যবহारিকদের জন্য लागतের अनुमान, por favor. \n<end_of_turn>\nTOTAL TIME ELAPSED: 5.58s\n\nGemma output:\n<start_of_turn>user\nकृपया केक के लिए एक ईमेल उत्तर सूची का अनुसरण करें और 10 लोगों के लिए लागत अनुमान पूछें, कृपया अंग्रेजी में प्रतिक्रिया मांगें<end_of_turn>\n<start_of_turn>model\nConsult the following email response list for a query and ask for cost estimates for 10 people, please request an English response.\n<end_of_turn>\nTOTAL TIME ELAPSED: 5.77s\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"gemma_lm.compile(sampler=\"top_k\") #default=1\n\ntext_gen(\"कृपया बेकर को ईमेल करें और सुनिश्चित करें कि वह ब्लैक फॉरेस्ट केक क्या बना सकता है और हमारी शादी की सालगिरह के लिए 4 मोमबत्तियां दे सकता है और लागत\")\ntext_gen(\"कृपया केक के लिए एक ईमेल उत्तर सूची का अनुसरण करें और 10 लोगों के लिए लागत अनुमान पूछें\")\ntext_gen(\"कृपया एक ईमेल उत्तर लिखें:\\n\\\"हैलो, मैं हमारी शादी की सालगिरह के लिए एक केक नंबर 3 ऑर्डर करना चाहूंगा?\")\ntext_gen(\"कृपया केक के लिए एक ईमेल उत्तर सूची का अनुसरण करें और 10 लोगों के लिए लागत अनुमान के बारे में पूछें, कृपया बांग्ला में प्रतिक्रिया के लिए पूछें।\")\ntext_gen(\"कृपया केक के लिए एक ईमेल उत्तर सूची का अनुसरण करें और 10 लोगों के लिए लागत अनुमान पूछें, कृपया अंग्रेजी में प्रतिक्रिया मांगें\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T16:15:43.618537Z","iopub.execute_input":"2025-02-04T16:15:43.619221Z","iopub.status.idle":"2025-02-04T16:16:17.052841Z","shell.execute_reply.started":"2025-02-04T16:15:43.619186Z","shell.execute_reply":"2025-02-04T16:16:17.051826Z"}},"outputs":[{"name":"stdout","text":"\nGemma output:\n<start_of_turn>user\nकृपया बेकर को ईमेल करें और पूछें कि क्या वह ब्लैक फॉरेस्ट केक बना सकता है और हमारी शादी की सालगिरह के लिए 4 मोमबत्तियाँ जोड़ सकता है और लागत अनुमान के बारे में पूछ सकता है<end_of_turn>\n<start_of_turn>model\nSubject: Black Forest Cake and Decorations for Wedding Anniversary\n\nHi [Baker's Name],\n\nDo you offer the Black Forest cake? We would like to order one for our wedding anniversary.  Also, could you please add 4 candles to it?\nAnd, could you tell us the approximate price?\n\nThanks,\n[Your Name]<end_of_turn>\nTOTAL TIME ELAPSED: 27.59s\n\nGemma output:\n<start_of_turn>user\nकृपया केक के लिए एक ईमेल उत्तर सूची का अनुसरण करें और 10 लोगों के लिए लागत अनुमान पूछें<end_of_turn>\n<start_of_turn>model\nविषय: केक के लिए अनुमान\n\nनमस्ते,\n\nआपके केक के रंग और स्वाद के बारे में विस्तार से बताएं, जैसे कि फाइव रंग के लिए पकाने की विधि और कल्पना की अनुमति.\n\n१. रंग\n२. तेल\n३. नमक\n४. सोडा\n५. पाकनिละเอียด\n<end_of_turn>\nTOTAL TIME ELAPSED: 3.43s\n\nGemma output:\n<start_of_turn>user\nकृपया एक ईमेल उत्तर लिखें:\n\"हैलो, मैं हमारी शादी की सालगिरह के लिए एक केक नंबर 3 ऑर्डर करना चाहूंगा?<end_of_turn>\n<start_of_turn>model\nHello! We would be delighted to order a cake for your anniversary. What flavor and size would you like? \n<end_of_turn>\nTOTAL TIME ELAPSED: 1.11s\n\nGemma output:\n<start_of_turn>user\nकृपया केक के लिए एक ईमेल उत्तर सूची का अनुसरण करें और 10 लोगों के लिए लागत अनुमान के बारे में पूछें, कृपया बांग्ला में प्रतिक्रिया के लिए पूछें।<end_of_turn>\n<start_of_turn>model\nসৈঠিক, 10জনের জন্য কেক এর মূল্যেরrough अनुमान,por favor, inquire:<end_of_turn>\nTOTAL TIME ELAPSED: 1.29s\n","output_type":"stream"}],"execution_count":31}]}