{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Goal:  PII De-identification Engine with Gradient Boosting Classifier","metadata":{}},{"cell_type":"code","source":"\n# Recommended package versions for compatibility\n!pip install -q scikit-learn==1.3.2 scipy==1.11.4 numpy==1.24.3\n\n# Install required packages\n!pip install -q -U  faker python-dateutil\nprint(\"Installation Complete\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T03:21:42.924794Z","iopub.execute_input":"2025-08-25T03:21:42.925201Z","iopub.status.idle":"2025-08-25T03:21:51.183698Z","shell.execute_reply.started":"2025-08-25T03:21:42.925168Z","shell.execute_reply":"2025-08-25T03:21:51.182165Z"}},"outputs":[{"name":"stdout","text":"Installation Complete\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Load Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom faker import Faker\nfrom datetime import datetime, timedelta\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nimport random","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T03:21:51.186234Z","iopub.execute_input":"2025-08-25T03:21:51.186526Z","iopub.status.idle":"2025-08-25T03:21:51.193603Z","shell.execute_reply.started":"2025-08-25T03:21:51.186501Z","shell.execute_reply":"2025-08-25T03:21:51.192342Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Core function","metadata":{}},{"cell_type":"code","source":"class FederatedPIIGradientBoost:\n    def __init__(self):\n        self.fake = Faker()\n\n    def create_data(self, n=500):\n        data = []\n        for _ in range(n):\n            name = self.fake.name()\n            age = random.randint(18, 85)  # Extended range to make age > 80 possible\n            dob = (datetime.now() - timedelta(days=age*365)).strftime('%Y-%m-%d')\n            phone = self.fake.phone_number()\n            risk_score = age * 1.2 + len(name) * 0.5 + (20 if age > 80 else 0) + random.uniform(10, 40)\n            \n            data.append({\n                'name': name,\n                'dob': dob,\n                'phone': phone,\n                'risk_score': risk_score\n            })\n        \n        return pd.DataFrame(data)\n\n    def deidentify(self, df):\n        \"\"\"Improved de-identification with better privacy protection\"\"\"\n        name_map = {name: self.fake.name() for name in df['name'].unique()}\n        \n        def shift_date(date_str):\n            # Random shift between 30-200 days for better privacy\n            shift_days = random.randint(30, 200)\n            date_obj = datetime.strptime(date_str, '%Y-%m-%d')\n            return (date_obj + timedelta(days=shift_days)).strftime('%Y-%m-%d')\n        \n        return df.assign(\n            name=df['name'].map(name_map),\n            dob=df['dob'].apply(shift_date),\n            phone=df['phone'].apply(lambda x: self.fake.phone_number())\n        )\n\n    def extract_features(self, df):\n        \"\"\"Extract features for machine learning\"\"\"\n        return np.column_stack([\n            df['name'].str.len(),\n            df['name'].str.split().str.len(),\n            2024 - pd.to_datetime(df['dob']).dt.year,\n            df['phone'].str.count(r'\\d'),  # Fixed regex pattern\n            df['phone'].str.len()\n        ])\n\n    def federated_train(self, num_clients=5, rounds=3):\n        print(\" FEDERATED LEARNING - GRADIENT BOOSTING\")\n        print(\"-\" * 45)\n\n        # Create client datasets\n        client_datasets = [self.create_data(2000) for _ in range(num_clients)]\n        print(f\"Created {num_clients} client datasets, each with 2000 records.\")\n\n        # Initialize global model parameters\n        global_model_params = None\n        test_accuracies = []\n\n        for round_num in range(1, rounds + 1):\n            local_models = []\n            print(f\"\\n--- Round {round_num} ---\")\n\n            for client_id, client_data in enumerate(client_datasets):\n                # Split data for training and local testing\n                train_size = int(0.8 * len(client_data))\n                train_data = client_data.iloc[:train_size].copy()\n                test_data = client_data.iloc[train_size:].copy()\n                \n                # De-identify data locally (PII never leaves client)\n                deidentified_train = self.deidentify(train_data)\n                \n                # Extract features from de-identified data for consistency\n                X_train = self.extract_features(deidentified_train)\n                y_train = (deidentified_train['risk_score'] > \n                          deidentified_train['risk_score'].median()).astype(int)\n                \n                # Scale features\n                scaler = StandardScaler()\n                X_train_scaled = scaler.fit_transform(X_train)\n                \n                # Train local model\n                local_model = GradientBoostingClassifier(\n                    n_estimators=100, \n                    max_depth=4, \n                    learning_rate=0.1,  # More reasonable learning rate\n                    random_state=42\n                )\n                local_model.fit(X_train_scaled, y_train)\n                \n                # Test local model\n                deidentified_test = self.deidentify(test_data)\n                X_test = self.extract_features(deidentified_test)\n                y_test = (deidentified_test['risk_score'] > \n                         deidentified_test['risk_score'].median()).astype(int)\n                X_test_scaled = scaler.transform(X_test)\n                \n                local_accuracy = local_model.score(X_test_scaled, y_test)\n                \n                # Store model for aggregation (in real, only parameters would be sent)\n                local_models.append(local_model)\n                \n                print(f\"  Client {client_id+1}: Local accuracy: {local_accuracy:.3f}\")\n\n            # Aggregate models (simplified - in practice would aggregate actual parameters)\n            if local_models:\n                # Average feature importances as a simple aggregation method\n                avg_feature_importances = np.mean([model.feature_importances_ \n                                                 for model in local_models], axis=0)\n                \n                # Store global model info\n                global_model_params = {\n                    'feature_importances': avg_feature_importances,\n                    'round': round_num\n                }\n            \n            print(f\"  Server: Aggregated parameters from {len(local_models)} clients\")\n\n        # Final results\n        feature_names = ['Name_Length', 'Name_Words', 'Age_Shifted', 'Phone_Digits', 'Phone_Length']\n        print(f\"\\nFinal Federated Model Results (after {rounds} rounds):\")\n        print(f\" PII Protection: 100% (PII never left client devices)\")\n        print(f\" Data Utility: Preserved through federated training\")\n        print(f\" Data Volume: {len(client_datasets[0]) * num_clients} total records\")\n        \n        if global_model_params:\n            print(\"\\n Aggregated Feature Importance (Global Model):\")\n            for name, importance in zip(feature_names, global_model_params['feature_importances']):\n                print(f\"  {name}: {importance:.3f}\")\n        \n        print(f\"\\nFederated Learning complete.\")\n        return global_model_params\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-25T03:21:51.194726Z","iopub.execute_input":"2025-08-25T03:21:51.195031Z","iopub.status.idle":"2025-08-25T03:21:51.219021Z","shell.execute_reply.started":"2025-08-25T03:21:51.195010Z","shell.execute_reply":"2025-08-25T03:21:51.217135Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Output","metadata":{}},{"cell_type":"code","source":"# Execute the corrected federated learning\nif __name__ == \"__main__\":\n    fl_system = FederatedPIIGradientBoost()\n    results = fl_system.federated_train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T03:21:51.220053Z","iopub.execute_input":"2025-08-25T03:21:51.220328Z","iopub.status.idle":"2025-08-25T03:22:01.165354Z","shell.execute_reply.started":"2025-08-25T03:21:51.220308Z","shell.execute_reply":"2025-08-25T03:22:01.164215Z"}},"outputs":[{"name":"stdout","text":" FEDERATED LEARNING - GRADIENT BOOSTING\n---------------------------------------------\nCreated 5 client datasets, each with 2000 records.\n\n--- Round 1 ---\n  Client 1: Local accuracy: 0.887\n  Client 2: Local accuracy: 0.887\n  Client 3: Local accuracy: 0.887\n  Client 4: Local accuracy: 0.905\n  Client 5: Local accuracy: 0.885\n  Server: Aggregated parameters from 5 clients\n\n--- Round 2 ---\n  Client 1: Local accuracy: 0.873\n  Client 2: Local accuracy: 0.875\n  Client 3: Local accuracy: 0.892\n  Client 4: Local accuracy: 0.905\n  Client 5: Local accuracy: 0.880\n  Server: Aggregated parameters from 5 clients\n\n--- Round 3 ---\n  Client 1: Local accuracy: 0.887\n  Client 2: Local accuracy: 0.907\n  Client 3: Local accuracy: 0.892\n  Client 4: Local accuracy: 0.902\n  Client 5: Local accuracy: 0.892\n  Server: Aggregated parameters from 5 clients\n\nFinal Federated Model Results (after 3 rounds):\n PII Protection: 100% (PII never left client devices)\n Data Utility: Preserved through federated training\n Data Volume: 10000 total records\n\n Aggregated Feature Importance (Global Model):\n  Name_Length: 0.027\n  Name_Words: 0.002\n  Age_Shifted: 0.942\n  Phone_Digits: 0.011\n  Phone_Length: 0.017\n\nFederated Learning complete.\n","output_type":"stream"}],"execution_count":7}]}